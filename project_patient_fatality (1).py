# -*- coding: utf-8 -*-
"""project-patient fatality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1takSTcT96yRPCyEM7cJbYZZzK_SwprFF

## **STEP 1: IMPORTING THE DATASET**
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
df=pd.read_csv('/content/drive/MyDrive/datasets/fatality.csv')
df

"""## **STEP 2: DATA PREPROCESSING**

**STEP 2.1 : HERE THE OUTPUT VARIABLES ARE 0 & 1 --->"CLASSIFICATION"**
"""

df['hospital_death'].unique()
#here the output values are 0,1 where 0---->death, 1-------->recovery

"""**STEP 2.2 :CHECKING FOR NULL VALUES IN THE**"""

df.isna().sum()

df['Unnamed: 83'].unique()
# here the column "Unnamed: 83 has only majority null values so for future data cleaning processes we should delete the column"

df.drop(['Unnamed: 83'],axis=1,inplace=True)
df

"""**STEP 2.3 : REMOVE NULL VALUES AND DUPLICATES**"""

df.drop_duplicates()
df.dropna(inplace = True)
len(df)

df

"""**STEP 2.4 : CHECKING THE DATA TYPE OF THE DATA**"""

df.dtypes

#------> SINCE SOME OBJECTS ARE IN FLOATS WE CAN ENCODE THE DATA

"""**STEP 2.5 : ENCODING THE OBJECT DATA**


"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
#since there are 84 columns we can use for loop to encode the columns with object type
for col in list(df.columns):
  if df[col].dtype == object:
        print("Column Name: \t\t", col)
        print("Unique Values: \t\t", list(df[col].unique()) )
        print("Length: \t\t", len(df[col].unique()))
        df[col]= le.fit_transform(df[col])
        print("Encoding Done Sucessfully!!! ")

df.dtypes

df.isna().sum()

df.isna().sum()
for col in list(df.columns):
  if df[col].isna().any():
    print("there are missing values in ",df[col])
  else:
    print("no missing values")

"""**STEP 2.6 : FEATURE EXTRACTION**"""

len(df.columns)
# so because of the high number of features we can use correlation for feature extraction

df

# Select the output column
y = df['hospital_death']

# Select the feature columns (excluding 'hospital_death')
X = df.drop(columns=['hospital_death'])

# Calculate correlations between features and the output
correlations = X.corrwith(y)

# Set a correlation threshold (e.g., 0.1)
correlation_threshold = 0.1

# Find columns with correlations below the threshold
low_correlation_columns = correlations[abs(correlations) < correlation_threshold].index

# Remove low-correlation columns from the DataFrame
df = df.drop(columns=low_correlation_columns)

# Now, 'data' contains only the columns with a correlation above the threshold
df

len(df.columns)

"""# **splitting the dataset into X,Y**"""

X=df.iloc[:,:-1]
y=df.iloc[:,-1]
print(X)
print(y)

df['hospital_death'].value_counts()

"""# **if we look at the data set the, dataset is highly imbalanced,so to mitigate this proposition we can use oversampling and undersampling**"""

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X_scaled=scaler.fit_transform(X)
X_scaled

"""## **OVERSAMPLING**"""

from imblearn.over_sampling import SMOTE #------>importing the library
oversample=SMOTE(random_state=1)
X_os,y_os=oversample.fit_resample(X_scaled,y)
y_os.value_counts()

from sklearn.model_selection import train_test_split
X_train_os,X_test_os,y_train_os,y_test_os=train_test_split(X_os,y_os,random_state=1,test_size=0.3)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay
knn=KNeighborsClassifier()
gb=GaussianNB()
dt=DecisionTreeClassifier()
rf=RandomForestClassifier()
ab=AdaBoostClassifier()
a=[]
mod=[knn,dt,gb,rf,ab]
for i in mod:
  i.fit(X_train_os,y_train_os)
  y_pred_i=i.predict(X_test_os)
  print("the accuracy_score by ",i,"is--------->",round(accuracy_score(y_pred_i,y_test_os)*100))
  print("**************",i,"***************","\n",classification_report(y_pred_i,y_test_os))
  print("ConfusionMatrixDisplay of ---->",i,ConfusionMatrixDisplay.from_predictions(y_pred_i,y_test_os))
  a.append(accuracy_score(y_pred_i,y_test_os)*100)

print(a)

import matplotlib.pyplot as plt
y = a
x = ["KNN",'DECISION-TREE','Naive-bayes','Random_forest','AdaBoost']
plt.bar(x, y)


plt.xlabel('Classification Algorithm')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Different Classification-Oversampling')


plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

"""# **undersampling**"""

X=df.iloc[:,:-1]
y=df.iloc[:,-1]

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X_scaled=scaler.fit_transform(X)
X_scaled

from imblearn.under_sampling import RandomUnderSampler
under_sampler=RandomUnderSampler(random_state=1)
X_us,y_us=under_sampler.fit_resample(X_scaled,y)
y_us.value_counts()

X_train_us,X_test_us,y_train_us,y_test_us=train_test_split(X_us,y_us,random_state=1,test_size=0.3)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay
knn=KNeighborsClassifier()
gb=GaussianNB()
dt=DecisionTreeClassifier()
rf=RandomForestClassifier()
ab=AdaBoostClassifier()
a1=[]
mod=[knn,dt,gb,rf,ab]
for i in mod:
  i.fit(X_train_us,y_train_us)
  y_pred_i=i.predict(X_test_us)
  print("the accuracy_score by ",i,"is--------->",round(accuracy_score(y_pred_i,y_test_us)*100))
  print("**************",i,"***************",classification_report(y_pred_i,y_test_us))
  print("ConfusionMatrixDisplay of ---->",i,ConfusionMatrixDisplay.from_predictions(y_pred_i,y_test_us))
  a1.append(accuracy_score(y_pred_i,y_test_us)*100)

print(a1)

import matplotlib.pyplot as plt
y = a1
x = ["KNN",'DECISION-TREE','Naive-bayes','Random_forest','AdaBoost']
plt.bar(x,y)


plt.xlabel('Classification Algorithm')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Different Classification -UnderSampling')


plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

"""# ** the Random_forest Classifier with over sampling shows the highest accuracy**

## **THE FINAL MODEL**
"""

X=df.iloc[:,:-1]
y=df.iloc[:,-1]

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X_scaled=scaler.fit_transform(X)

from imblearn.over_sampling import SMOTE #------>importing the library
oversample=SMOTE(random_state=1)
X_os,y_os=oversample.fit_resample(X_scaled,y)
y_os.value_counts()
from sklearn.model_selection import train_test_split
X_train_os,X_test_os,y_train_os,y_test_os=train_test_split(X_os,y_os,random_state=1,test_size=0.3)


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay,r2_score,precision_score,recall_score,f1_score

rf=RandomForestClassifier()
rf.fit(X_train_os,y_train_os)
y_pred_rf=rf.predict(X_test_os)
print("the accuracy_score by ",rf,"is--------->",round(accuracy_score(y_pred_rf,y_test_os)*100))
print("**************",rf,"***************","\n",classification_report(y_pred_rf,y_test_os))
print("ConfusionMatrixDisplay of ---->",rf,ConfusionMatrixDisplay.from_predictions(y_pred_rf,y_test_os))

acc=accuracy_score(y_pred_rf,y_test_os)
r2=r2_score(y_pred_rf,y_test_os)
pre=precision_score(y_pred_rf,y_test_os)
rec=recall_score(y_pred_rf,y_test_os)
f1=f1_score(y_pred_rf,y_test_os)
y = [acc,r2,pre,rec,f1]
x = ["ACCURACY","R2_SCORE",'PRECSION','RECALL','F1_SCORE']
plt.bar(x,y)


plt.xlabel('PARAMETERS')
plt.ylabel('VALUES')
plt.title('PERFORMANCE PARAMETERS OF CLASSIFICATION-MODEL')


plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have true labels (y_true) and predicted probabilities (y_score)
# Replace y_true and y_score with your actual data

fpr, tpr, thresholds = roc_curve(y_test_os, y_pred_rf)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

len(X.columns)

# parameters = ['age', 'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache',
#        'gcs_verbal_apache', 'heart_rate_apache', 'intubated_apache',
#        'temp_apache', 'ventilated_apache', 'd1_diasbp_min',
#        'd1_diasbp_noninvasive_min', 'd1_heartrate_max', 'd1_mbp_min',
#        'd1_mbp_noninvasive_min', 'd1_resprate_max', 'd1_spo2_min',
#        'd1_sysbp_min', 'd1_sysbp_noninvasive_min', 'd1_temp_min',
#        'h1_diasbp_min', 'h1_diasbp_noninvasive_min', 'h1_heartrate_max',
#        'h1_mbp_min', 'h1_mbp_noninvasive_min', 'h1_resprate_max',
#        'h1_spo2_min', 'h1_sysbp_min', 'h1_sysbp_noninvasive_min',
#        'd1_potassium_max', 'apache_4a_hospital_death_prob',
#        'apache_4a_icu_death_prob']

# parameter_values = []
# for parameter in parameters:
#     user_input = float(input(f"Enter the {parameter}: "))
#     parameter_values.append(user_input)

# # Now, the entered values are stored in the parameter_values array
# y_pred_rf=rf.predict(scaler.transform([parameter_values]))
# if y_pred_rf==0:
#   print("\n","there is no need to worry, recovery is emminent")
# else:
#   print("\n","chance of death is high")

# import joblib

# # Save your trained model to a file
# joblib.dump(rf, 'patient fatality project11.sav')